"Timestamp","Nume","Adresa web","Description","Mecanism","Formate de serializare","Limitări","Drepturi legale","Domeniu","Text and Data Mining","Implementare software de bază","Body","Github/Gitlab","Acronim","Namespaces","Tip după funcție","Scheme de metadate"
" 02/02/2023 16:13:06","American Archive of Public Broadcasting API","https://github.com/WGBH-MLA/AAPB2#api","Data from the AAPB is available via an API. At this moment the API is experimental: No key is required, but we also do not guarantee continued availability. The OAI-PMH feed can be used to harvest records for items available in the Online Reading Room. Please note that only records for items in the Online Reading Room can be harvested this way. We don't support all the verbs, or any formats beyond MODS.","HTTP, OAI-PMH","XML, JSON, JSONP",,,,,,,,,,,
" 02/02/2023 16:53:41","arXiv API Access","https://info.arxiv.org/help/api/index.html","The Cornell University e-print arXiv, hosted at arXiv.org, is a document submission and retrieval system that is heavily used by the physics, mathematics and computer science communities. It has become the primary means of communicating cutting-edge manuscripts on current and ongoing research. The open-access arXiv e-print repository is available worldwide, and presents no entry barriers to readers, thus facilitating scholarly communication. Manuscripts are often submitted to the arXiv before they are published by more traditional means. In some cases they may never be submitted or published elsewhere, and in others, arXiv-hosted manuscripts are used as the submission channel to traditional publishers such as the American Physical Society, and newer forms of publication such as the Journal for High Energy Physics and overlay journals. The primary interface to the arXiv has been human-oriented html web pages. The purpose of the arXiv API is to allow programmatic access to the arXiv's e-print content and metadata. The goal of the interface is to facilitate new and creative use of the the vast body of material on the arXiv by providing a low barrier to entry for application developers.","HTTP","XML",,"https://info.arxiv.org/help/license/index.html","mixed",,,,,,,,
" 02/02/2023 17:17:11","Springer Nature API portal","https://dev.springernature.com/","Springer Nature is a leading global scientific publisher of books and journals, delivering quality content through innovative information products and services. It publishes close to 500 academic and professional society journals. In the science, technology and medicine (STM) sector, the group publishes about 3,000 journals and 13,000 new books a year, as well as the largest STM eBook Collection worldwide. Springer Nature has operations in about 20 countries in Europe, the USA, and Asia, and more than 10,000 employees. We have created multiple APIs for developers to access our freely available content for noncommercial use: Springer Nature Meta API - Provides new versioned metadata for 14 million online documents (e.g., journal articles, book chapters, protocols). Springer Nature Metadata API - Provides metadata for 14 million online documents (e.g., journal articles, book chapters, protocols). Springer Nature Open Access API - Provides metadata and full-text content where available for more than 649,000 online documents from Springer Nature open access xml, including BMC and SpringerOpen journals. Springer Nature APIs provide a variety of different output formats, including XML and JSON. The Springer Nature API supports a number of different operations and return formats. These can be controlled by issuing RESTful requests to the API service at http://api.springernature.com. Text and Data Mining at Springer Nature: https://www.springernature.com/gp/researchers/text-and-data-mining","HTTP","XML, JSON, JSONP","https://www.springernature.com/gp/legal/general-terms-of-use/15067848","https://dev.springernature.com/terms-conditions","mixed",,,,,,,,
" 02/02/2023 17:28:43","Caselaw Access Project bu Harvard Law School","https://case.law/docs/site_features/api","The Caselaw Access Project (“CAP”) expands public access to U.S. law. Our goal is to make all published U.S. court decisions freely available to the public online, in a consistent format, digitized from the collection of the Harvard Law School Library. We created CAP's initial collection by digitizing roughly 40 million pages of court decisions contained in roughly 40,000 bound volumes owned by the Harvard Law School Library. The Harvard Law School Collection includes volumes published through 2018. The Harvard Law School Collection was digitized on site at Langdell Hall. Members of our team created metadata for each volume, including a unique barcode, reporter name, title, jurisdiction, publication date and other volume-level information. We then used a high-speed scanner to produce JP2 and TIF images of every page. A vendor then used OCR to extract the text of every case, creating case-level XML files. Key metadata fields, like case name, citation, court and decision date, were corrected for accuracy, while the text of each case was left as raw OCR output. In addition, for cases from volumes not yet in the public domain, our vendor redacted any headnotes. https://case.law/","HTTP","XML, JSON","https://case.law/docs/policies/access_limits","https://case.law/docs/policies/access_limits#commercial-licensing","Social sciences (economy, sociology, psychology, political science)",,,,,,,,
" 02/02/2023 17:53:23","Congress.gov API","https://api.congress.gov/","The beta Congress.gov Application Programming Interface (API) provides a method for Congress and the public to view, retrieve, and re-use machine-readable data from collections available on Congress.gov. This repository contains information on accessing and using the beta Congress.gov API, as well as documentation on available endpoints. https://github.com/LibraryOfCongress/api.congress.gov/","HTTP","XML, JSON","The rate limit is set to 1,000 requests per hour. By default, the API returns 20 results starting with the first record. The 20 results limit can be adjusted up to 250 results. If the limit is adjusted to be greater than 250 results, only 250 results will be returned. The offset, or the starting record, can also be adjusted to be greater than 0.","https://www.loc.gov/legal","mixed",,"CKAN",,,,,,
" 02/02/2023 18:01:16","Constellate","https://constellate.org/","Constellate is the text analytics service from the not-for-profit ITHAKA - the same people who brought you JSTOR and Portico. It is a platform for teaching, learning, and performing text analysis using the world’s leading archival repositories of scholarly and primary source content. Constellate enables schools of all sizes to teach data and text analytics with a learning platform that empowers faculty, librarians, and other instructors to educate a new generation of learners in text and data analysis. Our solution, centered on student and researcher success, provides text and data analysis capabilities and access to content from some of the world’s most respected databases in an open environment with a variety of teaching materials that can be used, modified, and shared. We envision a future where text and data analytics skills are being taught on Constellate in classes in all disciplines. https://constellate.org/news/open-open-access-portico-content https://constellate.org/docs/data-sources","HTTP","JSON, CSV",,"https://constellate.org/terms-and-conditions","mixed",,,,,,,,
" 02/02/2023 18:08:34","CORE API","https://core.ac.uk/services/api","CORE is the world’s largest aggregator of open access research papers from repositories and journals. It is a not-for-profit service dedicated to the open access mission. We serve the global network of repositories and journals by increasing the discoverability and reuse of open access content. We provide solutions for content management, discovery and scalable machine access to research. Our services support a wide range of stakeholders, specifically researchers, the general public, academic institutions, developers, funders and companies from a diverse range of sectors including but not limited to innovators, AI technology companies, digital library solutions and pharma. CORE collects, harmonises and enriches large quantities of both metadata and full text research articles from thousands of data providers. On top of this continuously growing corpus, we provide a truly unique API providing real-time machine access to both the metadata and full texts of research papers, enabling developers to build and run innovative applications on top of CORE's content.","HTTP","JSON","The API limits are designed to allow reasonable usage and we will change them depending on the load on our servers. You are able to monitor our current api limit by looking for our customised HTTP headers: X-RateLimitRemaining X-RateLimit-Retry-After X-RateLimit-Limit","https://core.ac.uk/terms","mixed",,,,,,,,
" 02/02/2023 18:19:30","Crossref REST API","https://www.crossref.org/documentation/retrieve-metadata/rest-api/","Crossref makes research objects easy to find, cite, link, assess, and reuse. We’re a not-for-profit membership organization that exists to make scholarly communications better. Our publicly available REST API exposes the metadata that members deposit with Crossref when they register their content with us. And it’s not just the bibliographic metadata either: funding data, license information, full-text links, ORCID iDs, abstracts, and Crossmark updates are in members’ metadata too. You can search, facet, filter, or sample metadata from thousands of members, and the results are returned in JSON.","HTTP, OAI-PMH","XML, JSON","There is a maximum row limit of 2000.","https://www.crossref.org/documentation/retrieve-metadata/rest-api/rest-api-metadata-license-information/","mixed","https://www.crossref.org/documentation/retrieve-metadata/rest-api/text-and-data-mining/",,,,,,,
" 03/02/2023 09:10:32","Dataverse","https://guides.dataverse.org/en/4.6/api/index.html","We encourage anyone interested in building tools to interoperate with the Dataverse to utilize our APIs. In 4.0, we require to get a token, by simply registering for a Dataverse account, before using our APIs (We are considering making some of the APIs completely public in the future - no token required - if you use it only a few times). SWORD stands for “Simple Web-service Offering Repository Deposit” and is a “profile” of AtomPub (RFC 5023) which is a RESTful API that allows non-Dataverse software to deposit files and metadata into a Dataverse installation. Client libraries are available in Python, Java, R, Ruby, and PHP. Introduced in Dataverse Network (DVN) 3.6, the SWORD API was formerly known as the “Data Deposit API” and data-deposit/v1 appeared in the URLs. The Search API supports the same searching, sorting, and faceting operations as the Dataverse web interface. The Data Access API provides programmatic download access to the files stored under Dataverse. More advanced features of the Access API include format-specific transformations (thumbnail generation/resizing for images; converting tabular data into alternative file formats) and access to the data-level metadata that describes the contents of the tabular files. Dataverse 4.0 exposes most of its GUI functionality via a REST-based API. Some API calls do not require authentication. Calls that do require authentication require the user’s API key. Currently there are client libraries for Python, R, and Java that can be used to develop against Dataverse APIs. We use the term “client library” on this page but “Dataverse SDK” (software development kit) is another way of describing these resources. They are designed to help developers express Dataverse concepts more easily in the languages listed below. For support on any of these client libraries, please consult each project’s README. Because Dataverse is a SWORD server, additional client libraries exist for Java, Ruby, and PHP per the SWORD API page.","SWORD","XML, JSON",,"https://theopenscholar.com/terms-service","mixed",,"Dataverse","The Institute for Quantitative Social Science","https://github.com/IQSS/dataverse",,,,
" 03/02/2023 09:29:00","Digital Public Library of America","https://pro.dp.la/developers/api-codex","The DPLA API is built with the same principles of openness in mind that underlie the broader project it supports. We wish to make the barrier to public access as low as possible, in order to facilitate broader engagement and model accessibility for other institutions similarly situated. All requests are presumed to be ‘friendly’ requests — that is, requests that respect the API’s operating parameters and capacity to deliver data.",,"JSON-LD","https://pro.dp.la/developers/policies",,"CHO (Cultural Heritage Organizations)",,,,"https://github.com/dpla","DPLA","dc, dcterms, dcmitype, dpla, ore, rdf, rdfs, skos, owl",,
" 03/02/2023 09:46:49","Europeana API","https://pro.europeana.eu/page/apis","Europeana APIs allow you to build applications that use the wealth of our collections drawn from the major museums and galleries across Europe. Their scope includes millions of cultural heritage items (from books and paintings to 3D objects and audiovisual material) that celebrate around 4,000 cultural institutions across Europe. If you want to search Europeana in an simple way (for instance 'give me all results for the word cat), you can then use the Search API. But if you are looking for a way to delve into the structured metadata of Europeana (For instance, to ask the question ""What are all the French 18th-century painters with at least five artworks available through Europeana'), then the SPARQL service is more appropriate. On the other, if you want to get all the metadata associated with a single item, then you can use the Record API. It also possible to obtain a larger amount of metadata and ultimately harvest the complete Europeana repository by using the OAI-PMH Service. Regarding contextual information that is associated to items, we also offer an Entity API that gives you access to information such as Topics, Persons and Places. Lastly, if you want to contribute information about the items that are available on Europeana, you can do it via the Annotations API.","HTTP, OAI-PMH, SPARQL","XML, JSONP, JSON-LD, RDF",,"https://www.europeana.eu/en/rights/terms-of-use",,,,,"https://github.com/europeana",,"dc, dcterms, ore, rdf, skos, owl, cc, foaf, rdaGr2, wgs84, edm",,
" 03/02/2023 10:19:26","HathiTrust Digital Library","https://www.hathitrust.org/data","The HathiTrust collection is composed of works from over 50 different libraries located in the United States and around the world. Bibliographic records represent many different cataloging practices and may even be in different languages. You can use the HathiTrust APIs to query and retrieve data when you have a known identifier. HathiTrust APIs are not search APIs (e.g., where you use a keyword to search across the collection). You can use the Bibliographic API to do real-time querying against the HathiTrust collection and to retrieve a limited number of bibliographic records. Using a variety of common identifiers (e.g., ISBN, LCCN, OCLC, etc.) as well as HathiTrust identifiers, you can retrieve information about any works associated with those identifiers. The API can provide you with brief or full bibliographic records. The Data API allows you to retrieve page images, OCR text for individual pages, and METS metadata. To retrieve the OCR for more than a few volumes, we recommend that you request a dataset. Restrictions apply.","OAI-PMH","XML, JSON, JSONP","https://www.hathitrust.org/acceptable-use","https://www.hathitrust.org/copyright",,"https://analytics.hathitrust.org/",,,,,,,
" 03/02/2023 10:25:07","IEEE Xplore API","https://developer.ieee.org/getting_started","Query and retrieve metadata records including abstracts for more than 5 million documents in IEEE Xplore® including Journals, Conference Proceedings, Books, Courses and Standards. The Metadata API supports both simple and Boolean searches.","HTTP","XML, JSON",,"https://developer.ieee.org/API_Terms_of_Use2","mixed","TDM is permitted for non-commercial research purposes only and requires an active IEEE Xplore institutional subscription.",,"IEEE",,,,,
" 03/02/2023 10:33:29","Internet Archive Developer Portal","https://archive.org/developers/index.html","This portal contains information to help you access data from, integrate with, or contribute to the Internet Archive. Some of the things that you can do with the information on this Developer Portal are: Use the APIs and services to read data, for example, see what the first edition of Shakespeare’s Hamlet looked like. Update or modify data, for example, change the metadata of an item. Upload items, for example, add a video file to the Internet Archive.","HTTP","XML, JSON","The Internet Archive does not assert any new copyright or other proprietary rights over any of the material in its database. The legal issues for community projects like the Internet Archive can be confusing, but the Internet Archive aims to make available database that can be freely used.","https://archive.org/about/terms.php","mixed",,,,,,,,
" 03/02/2023 10:48:48","The Lens","https://docs.api.lens.org/index.html","The Lens serves linked open knowledge artefacts and metadata with tools to inform effective, efficient and equitable problem solving. The required knowledge falls into many silos of specialization, ranging from scholarly research and patent knowledge to policy, laws, regulations, investment, social norms and business data. These silos create barriers to visualizing effective partnerships, opportunities, risks and trajectories (PORTs), making inclusive problem-solving slow, difficult and expensive. The Lens, the flagship project of the social enterprise Cambia, seeks to source, merge and link diverse open knowledge sets, including scholarly works and patents, to inform discovery, analysis, decision making and partnering on a human-centered user experience built on an open web platform, Lens.org, with toolkits designed to optimize institutional effectiveness in problem solving. With over 20 years of development, supported by prominent philanthropic organizations, The Lens ingests, cleans, aggregates, normalizes and serves over 225+ million scholarly works, 127+ million global patent records, and more than 370+ million patent sequences, with rich metadata including the people and institutions that generate this knowledge and the linkages between them, drawn from diverse data sources. The Lens architecture is built around the Lens MetaRecord. 'Knowledge artefacts' - including scholarly works and patents, exist in a constellation of forms, timelines, degrees of access and quality. By integrating multiple identifiers and sources to provide an open MetaRecord, the best metadata can be assembled, normalized and exposed, while maintaining provenance and linkages. All of the software that comprises the Lens platform itself is Open Source and free to use. The code base is not proprietary code1. The following is a brief overview of the main technologies and frameworks that make up the Lens applications: Lens servers run within the Amazon EC2 cloud-computing platform We use PostgreSQL, MySQL and MongoDB databases Elasticsearch and Apache Lucene are used for text search NGINX and Apache HTTP Servers are used for proxies and load balancing The backend applications are powered by Apache Tomcat and Gunicorn Images and static resources are stored and served using Amazon S3/CloudFront The open source software used in the PatSeq facility includes: PatSeq Finder: NCBI BLAST+ PatSeq Analyzer: opencb/genome-maps, opencb/CellBase, BWA, BLAT, Apache Solr","HTTP, Swagger","JSON",,"https://about.lens.org/lens-api-terms-of-use/","mixed",,,,,,,"Agregator",
" 03/02/2023 11:09:02","JSON/YAML for LoC.gov","https://loc.gov/apis","The Library of Congress makes three different loc.gov APIs available to the public: JSON/YAML for loc.gov: The loc.gov API provides structured data about Library of Congress collections. The API was originally designed to power the loc.gov website, but in addition to providing HTML for the website it can provide a wealth of information in JSON format. Sitemaps: A sitemap provides information on the relationships between the pages, videos, images and other resources on a website. They are primarily used to inform search engines about the pages that are available for crawling. It is expressed as an XML file listing URLs and their associated metadata. Conventionally, sitemaps are not described as APIs but it's convenient to discuss them in relationship to other LC APIs since they are also used for automated interactions, especially by web crawlers. Microservices: A microservice is a limited-purpose computer system written to carry out a specific role and using a lightweight API. The three microservices described on this page fall into three categories: Text Services, Image Services, and Streaming Services. Text Services provides an API for accessing full text OCR, word coordinates and context snippets on loc.gov. Image Services provides an IIIF-compliant API for accessing and manipulating images from the Library of Congress. Streaming Services provides an audio and video (A/V) delivery API for the Library of Congress. The loc.gov API provides structured data about Library of Congress collections in the JSON and YAML formats. Software programs routinely access the JSON API to keep the loc.gov website updated as new digital content is added to the Library's collections. For example, JSON data is used to build loc.gov pages for items (loc.gov/item), collections (loc.gov/collections/), searches (loc.gov/search/), and more. However, in addition to being a resource for the computer applications powering the loc.gov website, the API can be used by developers, digital librarians, and researchers to retrieve digital collections information formatted as JSON or YAML data directly.","HTTP","JSON, YAML","The API is accessible to the public with no API key or authentication required, however, rate limiting is strongly encouraged. Requests that exceed the rate which loc.gov can successfully accommodate will be blocked to prevent a denial of service. The current rate limits are: Newspapers endpoint: Burst Limit 	20 requests per 1 minute, Block for 5 minutes Crawl Limit: 	20 requests per 10 seconds, Block for 1 hour Item endpoint: Burst Limit: 	10 requests per 10 seconds, Block for 5 minutes Crawl Limit 	200 requests per 1 minute, Block for 1 hour Resource endpoint: Burst Limit: 	40 requests per 10 seconds, Block for 5 minutes Crawl Limit 	200 requests per 1 minute, Block for 1 hour Collections, format, and other endpoints: Burst Limit 	20 requests per 10 seconds, Block for 5 minutes Crawl Limit	80 requests per 1 minute, Block for 1 hour Note about deep paging limits There is also a limit on how deeply a user can paginate into results. Paging past the 100,000th item in a search result is not supported at this time. In some searches, responses may fail before 100,000 items. To avoid this limit, we suggest filtering results using facets.","https://www.loc.gov/legal/","CHO (Cultural Heritage Organizations)",,,"Library of Congress",,,,,
" 03/02/2023 11:30:10","Chronicling America","https://chroniclingamerica.loc.gov/about/api/","Search America's historic newspaper pages from 1777-1963 or use the U.S. Newspaper Directory to find information about American newspapers published between 1690-present. Chronicling America is sponsored jointly by the National Endowment for the Humanities and the Library of Congress Chronicling America provides access to information about historic newspapers and select digitized newspaper pages. To encourage a wide range of potential uses, we designed several different views of the data we provide, all of which are publicly visible. Each uses common Web protocols, and access is not restricted in any way. You do not need to apply for a special key to use them. Together they make up an extensive application programming interface (API) which you can use to explore all of our data in many ways. The directory of newspaper titles contains nearly 140,000 records of newspapers and libraries that hold copies of these newspapers. The title records are based on MARC data gathered and enhanced as part of the NDNP program. Searching the title records is possible using the OpenSearch protocol. ","HTTP","JSON, JSONP, RDF",,"https://www.loc.gov/legal/",,,,,"https://github.com/LibraryofCongress/chronam",,"dc, dcterms, ore, owl",,
" 03/02/2023 11:32:53","Print & Photographs Online Catalog - JSON HTTP API",,"The Library of Congress' recently re-released Print & Photographs Online Catalog (http://www.loc.gov/pictures/) provides a json serialization of the request-scoped state used to create every html page. This immediately enables PPOC to serve as a simple API for developers to make use of while building other applications, integrating Library data in new and innovative ways.","HTTP","JSON",,"https://www.loc.gov/legal/","CHO (Cultural Heritage Organizations)",,,"Library of Congress",,"PPOC",,,
" 03/02/2023 11:43:41","The New York Times Developer Network","https://developer.nytimes.com/apis","Our APIs (Application Programming Interfaces) allow you to programmatically access New York Times data for use in your own applications. Our goal is to facilitate a wide range of uses, from custom link lists to complex visualizations. Why just read the news when you can hack it? NYT currently has ten public APIs: Archive, Article Search, Books, Most Popular, Semantic, Times Newswire, TimesTags, and Top Stories.","HTTP","JSON","4,000 requests per day and 10 requests per minute. You should sleep 6 seconds between calls to avoid hitting the per minute rate limit.","https://developer.nytimes.com/terms","mixed","https://nytlicensing.com/data-solutions/",,"The New York Times",,,,,
" 03/02/2023 11:47:40","OECD data for developers","https://data.oecd.org/api/","The OECD has application programming interfaces (APIs) that provide access to datasets in the catalogue of OECD databases. The APIs allow you to query the data in several ways, using parameters to specify your request so that you can create innovative software applications which use OECD datasets. The APIs are available in JSON and XML formats. You can read more about the technical specifications of the data interfaces for JSON format in the API Documentation (JSON) and in API documentation (SDMX-ML).","HTTP","XML, JSON",,"https://www.oecd.org/termsandconditions/",,,,,,,,,
" 03/02/2023 11:55:05","Open Researcher and Contributor ID","https://info.orcid.org/documentation/features/public-api/","ORCID offers a public API that allows organizations that are not ORCID members to connect their systems and applications to the ORCID registry with machine-to-machine communications. The API is a restful API and supports both XML and JSON. Member organizations can use the member API to request and obtain permission to read trusted data on their researchers ORCID records. The organization can ask researchers to grant them specific permission to read limited-access information at the same time that they request permission to collect ORCID iD’s. Once the researcher has granted permission, the trusted organization will be able to use the member API to read the ORCID record and read information that the researcher has set as visible to trusted parties in addition to the information set as visible to everyone.","HTTP","XML, JSON, CSV","https://github.com/ORCID/ORCID-Source/tree/development/orcid-api-web#api-limits","https://info.orcid.org/terms-of-use/","mixed",,,,"https://github.com/ORCID/ORCID-Source/tree/development/orcid-api-web/tutorial","ORCID",,,
" 04/02/2023 15:37:20","Joint Research Centre Data Catalogue","https://data.jrc.ec.europa.eu/","The Joint Research Centre (JRC) is the European Commission's in-house science service which employs scientists to carry out research in order to provide independent scientific advice and support to policies of the European Union. A dedicated JRC Data Policy was prepared to complement the JRC Policy on Open Access to Scientific Publications and Supporting Guidance, and to promote open access to research data in the context of Horizon 2020. Important policy commitments and the relevant regulatory basis within the European Union and the European Commission include the following documents: Commission Decision on the reuse of Commission documents (2011/833/EU) Commission Communication on better access to scientific information (COM/2012/0401 final) Commission Communication on a reinforced European research area partnership for excellence and growth (COM/2012/0392 final) Commission Recommendation on access to and preservation of scientific information (2012/417/EU) In this catalogue, you can find an inventory of data produced by the JRC in accordance with the JRC Data Policy. The content is continuously updated and shall not be seen as a complete inventory of JRC data. Currently, the inventory describes only a small subset of JRC data. The catalogue organises its content in dataset collections. A dataset collection is a set of metadata records about datasets that are related to each other according to some criterion. One way to access the data is to use the REST API. Most of the portal core functionalities are available through the application programming interface (API), which encompasses most of what you can do with the web interface. The information retrieved can then be used by an external code to transform, update or reference and provide new input for further calls to the API. The following API are currently available: CKAN action API, RPC-style API that exposes some legacy features to existing API clients. ODCAT action API, RPC-style API that we recommend to use by new API clients. You can teach your application to communicate with the catalogue using OpenAPI specification. Additionally, metadata can be retrieved in RDF or JSON formats directly from a collection or a dataset page by adding accordingly .RDF or .RJ extension to the page url, for instance https://data.jrc.ec.europa.eu/collection/emm.rdf. RDF data model The reference standard used for Metadata is DCAT-AP (DCAT application profile for data portals in Europe), that is actually the de facto EU metadata interchange format, extended in order to meet the identified requirements and needs for scientific data. However, since JRC data are multi-disciplinary, and each discipline uses specific metadata standard, as well as specific controlled vocabularies for annotating metadata records, the JRC Data Catalogue infrastructure is designed to re-use as much as possible existing metadata, without requiring the original records to be re-created based on the reference standard (namely, DCAT-AP). https://data.jrc.ec.europa.eu/docs/index.html","HTTP, Swagger","JSON",,"https://data.jrc.ec.europa.eu/about","mixed",,"CKAN","Joint Research Centre of the European Commission",,,,"Agregator","DCAT-AP"
" 05/02/2023 17:56:28","data.europa.eu - The official portal for European data","https://data.europa.eu/api/hub/search/","The portal is a central point of access to European open data from international, European Union, national, regional, local and geodata portals. It consolidates the former EU Open Data Portal and the European Data Portal. The portal is intended to: give access and foster the reuse of European open data among citizens, business and organisations; promote and support the release of more and better-quality metadata and data by the EU's institutions, agencies and other bodies, and European countries; educate citizens and organisations about the opportunities that arise from the availability of open data. The two former portals EU Open Data Portal and European Data Portal, launched respectively in 2012 and 2015, were originally established on the basis of Directive 2003/98/EC to promote accessibility to and the reuse of public sector information. The successor directives Directive 2013/37/EU and Directive (EU) 2019/1024 confirmed and extended its action. It invites all the EU Member States to publish their public data resources, such as open datasets, and to make them accessible to the public whenever possible. Currently in its third iteration, the portal merges the activities of the European Data Portal (which focused exclusively on EU Member States and other European countries) and of the EU Open Data Portal (which served data from the EU institutions, agencies, and bodies) into one. It is funded by the EU and managed by the Publications Office of the European Union. The Directorate-General for Communications Networks, Content and Technology of the European Commission is responsible for the implementation of EU open data policy, in collaboration with the project's management. The metadata catalogue of datasets can be explored through a search engine (data tab), through a map for geospatial data or through a SPARQL endpoint and API endpoint. The 'data providers' (EU institutions, agencies and EU bodies, Member States, and other European countries) are autonomous in publishing their metadata (which gives you access to their data) in data.europa.eu. The portal also publishes datasets of other European countries and organisations beyond the EU. The portal is updated when new datasets and content are available. data.europa.eu provides the following APIs to read our metadata: Search: https://data.europa.eu/api/hub/search/ SPARQL: https://data.europa.eu/sparql Registry: https://data.europa.eu/api/hub/repo/ Use cases: https://data.europa.eu/en/export-use-cases MQA: https://data.europa.eu/api/mqa/cache/ SHACL metadata validation: https://data.europa.eu/api/mqa/shacl/","HTTP, OAI-PMH, SPARQL","JSON, JSON-LD",,"The choice of a license should be discussed with the data provider. Following the EC decision from 2019, Directorates-General should try to publish their reusable content under CC BY, but for publications ordered before 2019 things are not as clear. In any case it's their decision.","mixed",,"CKAN","Publications Office of the European Union","https://gitlab.com/european-data-portal",,,"Agregator",
" 06/02/2023 20:03:27","Elsevier Research Products APIs","https://dev.elsevier.com/","Elsevier's RESTful APIs provide access to content from platforms like ScienceDirect, Scopus and Engineering Village for various use cases. These use cases are governed by the policies outlined on this page. These policies define how clients are allowed to use content retrieved through Elsevier's APIs. Get programmatic access to: Citation data, metadata, and abstracts from scholarly journals, as indexed by Scopus, Elsevier's citation database.kl Full-text journals and books published by Elsevier on the ScienceDirect full-text platform. Research metrics available on SciVal, Elsevier's platform for research performance benchmarking. Engineering resources available on Engineering Village.. Curated abstracts, indices and other metadata indexed by Embase, Elsevier's biomedical abstract and indexing database. Reactions, chemical structures & chemistry information from Reaxys, Elsevier's expert curated chemistry database. Safety data, efficacy data, pharmacokinetic, metabolizing enzymes and transporter data from PharmaPendium, Elsevier's fully-searchable database containing data extracted from FDA and EMA drug approval documents, as well as FAERs, to inform drug development decisions. ScienceDirect and Scopus use two different databases. ScienceDirect contains full text articles from journals and books, primarily published by Elsevier but also including some hosted societies. Scopus indexes metadata from abstracts and references of thousands of publishers, including but not limited to Elsevier, and builds additional functionality on top of that data: citation matching, various metrics, author profiles, and affiliation profiles. OpenAccess Article Search is powered by ScienceDirect Search API and Serial Title API. The search query is constructed such that search terms entered by the user are concatenated with OpenAccess field restriction. The API response is processed by the browser in accordance with XSLT to display article citations returned by search API along with respective publication cover image returned by Serial Title API. For convenience cover images link to the corresponding publication homepage on ScienceDirect. Article Search via API is similar to OpenAccess Article search with exception that search is not restricted to OpenAccess content. Article Search is powered by ScienceDirect Search API and Serial Title API. The API response is processed by the browser in accordance with XSLT to display article citations returned by search API along with respective publication cover image returned by Serial Title API. For convenience cover images link to the corresponding publication homepage on ScienceDirect. Scopus Search is powered by Scopus Search API. The search query is constructed by scoping user entered terms to Scopus all field. The API response is processed by the browser in accordance with XSLT to display article citations and cited by counts returned by Scopus Search API.","HTTP, Swagger","XML, JSON","https://dev.elsevier.com/api_service_agreement.html","https://www.elsevier.com/legal/elsevier-website-terms-and-conditions","Natural sciences (chemistry, earth sciences, physics, cosmology, biology, paleontology), Social sciences (economy, sociology, psychology, political science, law), Humanities (linguistics, philology, art, literature, history), Applied science (medical science, informatics, space)","https://dev.elsevier.com/tdm_service.html",,,"https://github.com/ElsevierDev",,,,
" 06/02/2023 20:28:17","F1000Research ","https://f1000research.com/developers","This API allows anyone to download the XML and PDF of specific articles as well as to download links to the XML of the entire corpus of articles. The content is indexed using Apache Solr, which is open-source software that allows simple queries to be built in order to search and retrieve the content required. Each article is assigned a unique DOI that can be resolved at doi.org. An example of a DOI for an F1000Research article is 10.12688/f1000research.13256.2. Solr queries will return the DOIs of articles that match the search. See below for details of how to build a Solr query, and the parameters and terms that you can use to refine your search.","HTTP","XML","Limitations on requests Frequency of requests Please note, user can only make 100 requests per 60 seconds; if the requests exceed this, an unauthorized status 401 will be returned. Number of results per request There is a limit of 100 results per request.","© 2012-2023 F1000 Research Ltd.","mixed",,,,,,,"Agregator",
" 07/02/2023 07:03:08","Clarivate Developer Portal","https://developer.clarivate.com/","Cortellis Labs A showroom of the Cortellis APIs collection, its diverse content sets, and analytical capabilities. The Derwent API provides programmatic access to the world's most trusted global patent data. An API for querying ScholarOne Manuscripts for more information on a manuscript, authors, and other roles, and a notification service to push out information on status changes, submission, and decision via HTTPS pathways. Web of Science™ data is uniquely selective, based on an independent editorial process combined with over 50 years of essential, accurate and unique curation, resulting in our unparalleled data structure. Every article from every journal has been indexed, creating a comprehensive and complete data network. Web of Science APIs seamlessly integrate the world’s best publication and citation data, giving you flexibility and control of the highest-quality and normalized bibliometric indicators. Power and enrich your institution’s analysis and assessment capabilities. The Web of Science™ Journals API provides REST-based programmatic access to the Journal Citation Reports™, with JSON output format and advanced search and filter capabilities. Journal Citation Reports is the only journal report of its kind that is both complete and editorially selective; it contains all the data required to understand the components that index the value and impact of each journal. APIs: CDDI SUSHI REST API Converis Read API Data Feed API ESTI service apis InCites Document Level Metrics API incites-bna-user-db-service Intellectual Property (IP) Data API Web of Science API Expanded Web of Science API Lite Web of Science Journals API Web of Science Researcher API Web of Science Starter API Web of Science SUSHI API Wos Trend Explorer funder BE API The structured data are curated by a global team of experts who continuously evaluate and select the collections of journals, books, and conference proceedings covered in the Web of Science Core Collection to ensure accuracy in evaluating journal impact. These expert insights enable you to explore the key drivers of a journal's value, making better use of the vast body of data and metrics available in the Journal Citation Reports, including the Journal Impact Factor (JIF) and other ranking analysis.","HTTP, Swagger","XML, JSON","https://developer.clarivate.com/content/api-usage/view","https://clarivate.com/legal-center/terms-of-business/","mixed",,,,"https://github.com/clarivate/wosjournals-javascript-client",,,"Metacatalog",
" 07/02/2023 07:13:01","Wiley Online Library","https://olabout.wiley.com/WileyCDA/Section/id-829771.html","Wiley is keen to encourage innovative uses of the content we publish, and supports customers who wish to perform text and data mining (TDM) on Wiley content. We are committed to developing tools and services that will enable subscribers to carry out TDM in the most efficient and effective manner, as well as to providing straightforward access to content for TDM purposes. Academic subscribers can perform TDM under license (or in accordance with statutory rights in the UK) on subscribed content for non-commercial purposes at no extra cost. Corporate subscribers should contact their account manager to discuss options available for content access and delivery. In order to maximize platform stability and security for all users, we ask that access to content for TDM purposes takes place through an approved API service, rather than through crawling Wiley Online Library. Our preferred access solution for TDM is the Crossref Text and Data Mining Service. Academic subscribers can register with Crossref and will then be able to access subscribed content once they have accepted the Wiley click-through TDM license and received an API token. Further details on the Crossref Text and Data Mining Service are available at http://tdmsupport.crossref.org/researchers/.","HTTP","XML, JSON",,"https://olabout.wiley.com/WileyCDA/Section/id-826542.html","Natural sciences (chemistry, earth sciences, physics, cosmology, biology, paleontology), Social sciences (economy, sociology, psychology, political science, law), Humanities (linguistics, philology, art, literature, history), Applied science (medical science, informatics, space)","https://www.crossref.org/documentation/retrieve-metadata/rest-api/text-and-data-mining-for-researchers/",,"Wiley",,,,,
" 20/02/2023 18:57:05","National Data Catalog","https://data.norge.no/dataservices","The National Data Catalog – data.norge.no is the public website providing an overview of descriptions of datasets, concepts, APIs and information models. The content is supplied by various establishments, both public and private. The Digitalization Agency is responsible for the operation and development of the website.","HTTP",,,"https://www.regjeringen.no/no/dokumenter/retningslinjer-ved-tilgjengeliggjoring-av-offentlige-data/id2536870/ Creative Commons CC-BY 4.0 Norwegian Licence for Open Government Data (NLOD)","mixed",,,"Digitaliseringsrundskrivet",,,,"Metacatalog, Agregator","DCAT-AP"
" 21/02/2023 21:20:17","Crossref Unified Resource API","https://api.crossref.org/swagger-ui/index.html","The Crossref REST API is one of a variety of tools and APIs that allow anybody to search and reuse our members' metadata in sophisticated ways. If you read nothing else, please at least look at the API TIPs document and the ""Etiquette"" section of this document. It will save you (and us) much heartburn. https://www.crossref.org/services/metadata-retrieval/ https://www.crossref.org/documentation/retrieve-metadata/rest-api/ You can query our API to find answers to questions about our metadata. Copy and paste these URLs into a browser to find out what has been registered with us. You can modify the parameters in each URL, for example: if https://api.crossref.org/members?rows=100 gives the first 100 accounts, modify it to https://api.crossref.org/members?rows=200 to give the first 200. There is a maximum row limit of 2000. How many accounts do wehave? (This includes members and others, both inactive and active) https://api.crossref.org/members?rows=0 Who are they? Let’s look at the first 100 accounts https://api.crossref.org/members?rows=100 And the second 100 accounts https://api.crossref.org/members?rows=100&offset=100 How many records do we have? https://api.crossref.org/works?rows=0 What content types do we have? https://api.crossref.org/types How many journal article DOIs do we have? https://api.crossref.org/types/journal-article/works?rows=0 How many conference proceedings records do we have? https://api.crossref.org/types/proceedings-article/works?rows=0 How can I see all the records registered under a given prefix? https://api.crossref.org/prefixes/10.21240/works?select=DOI&rows=1000 And how can I see all the works registered under a given prefix? https://api.crossref.org/prefixes/10.35195/works If your prefix has more than 1,000 DOIs registered, not all of them will display on one page, so it’s best to query for them using machine retrieval from the REST API But eventually you will probably want to start looking at metadata records. Let’s search for records that have the word “blood” in the metadata and see how many there are. https://api.crossref.org/works?query=blood&rows=0 Let’s look at some of the results. https://api.crossref.org/works?query=%22blood%22& Now let’s look at one of the records https://api.crossref.org/works/10.1155/2014/413629. Interesting. The record has ORCID iDs, full-text links, and license links. You need license and full-text links to text and data mine the content. How many works have license information? https://api.crossref.org/works?filter=has-license:true&rows=0 How many license types are there? https://api.crossref.org/licenses?rows=0 OK, let’s see how many records with the word “blood” in the metadata also have license information and full-text links https://api.crossref.org/works?filter=has-license:true,has-full-text:true&query=blood&rows=0 And here’s how to look up the XML behind a DOI https://api.crossref.org/works/10.5555/487hjd.xml. Remove .xml to see results in JSON https://api.crossref.org/works/10.5555/487hjd.","HTTP, OAI-PMH, Swagger","XML, JSON",,"https://www.crossref.org/documentation/metadata-plus/#00342 Service level agreement (SLA) Crossref will maintain an aggregated, average uptime for all of the interfaces that together comprise the Crossref Metadata Service of 99.5%, reported on a monthly basis. Crossref will provide technical support to Subscriber through Crossref’s existing support channels as requested by Subscriber, and will provide a response within one (1) business day to support requests received during normal working hours in the United States and the United Kingdom. “Business days” do not include weekends or legal holidays in the United States and the United Kingdom. “Response” means that support requests will be acknowledged. The time required for resolution will depend upon the nature of the request. Agreement and fees for Metadata Plus Learn more about the Metadata Plus service agreement, and fees and pricing tiers for Metadata Plus.","mixed","https://www.crossref.org/documentation/retrieve-metadata/rest-api/text-and-data-mining/",,"Crossref",,,,"Agregator",
" 21/02/2023 23:02:41","Data.Bibliotheken.nl","http://data.bibliotheken.nl/","The KB's extensive digital collections make it possible to use software to analyse large quantities of text, structured data or images from several centuries, in search of historical patterns or breaks. When did a word disappear from the Dutch language? When did the majority of illustrations in Dutch newspapers become photographs? When did people write positively or negatively about certain concepts or people? All that data also offers unprecedented opportunities for making connections with other collections or data, or for bringing the history of the Netherlands to life through apps or online visualisations. Access to open and non-open data The method of access differs per collection or file and depends, among other things, on the legal status, the technical possibilities and the type of (meta)data. New forms of access are being developed on a regular basis. Usually open data (i.e. not protected by copyright) can be downloaded without prior registration. For non-open data please contact dataservices@kb.nl and briefly describe what data you need and why. There is often an arrangement for research purposes. However, you may first have to sign a contract or access to the data may be limited to the KB building. http://data.bibliotheken.nl/sparql","HTTP, SPARQL",,,"CC0","CHO (Cultural Heritage Organizations)",,,"Koninklijke Bibliotheek",,,"rdf","Agregator",
" 06/03/2023 16:12:29","BnF API et jeux de données","https://api.bnf.fr/","Le portail BnF API et jeux de données décrit et documente l’ensemble des API (Application Programming Interface, interface de programmation applicative) qui permettent d’interroger et de récupérer les métadonnées des catalogues et les collections numérisées de la BnF (notamment BnF catalogue général, data.bnf.fr, Gallica). Pour faciliter l’accès aux données et leur utilisation, des jeux de données (images et textes, métadonnées, statistiques) sont directement téléchargeables via le portail.(https://www.bnf.fr/fr/portail-bnf-api-et-jeux-de-donnees) data.bnf.fr rassemble les données issues des différentes bases et catalogues de la BnF pour y donner un accès fédéré par auteurs, oeuvres, thèmes, lieux et dates. Les données de data.bnf.fr sont enrichies par des alignements avec d'autres données publiées sur le Web, comme Wikidata ou DBpedia. Elles sont exprimées selon les standards du Web sémantique et sont récupérables au format RDF (XML, NT, N3) et JSON ou JSON-LD. data.bnf.fr utilise des données produites dans des formats divers, notamment Intermarc pour les livres imprimés, XML-EAD pour les archives et manuscrits, et Dublin Core pour la bibliothèque numérique Gallica. Ces données sont modélisées, regroupées, enrichies par des traitements automatiques et publiées selon le langage du Web sémantique, le RDF (Ressource description framework). data.bnf.fr expose l'ensemble des données de bonne qualité des catalogues et bases de données de la BnF, à savoir les données relatives aux autorités (personnes, organisations, thèmes, oeuvres, lieux) validées par un expert et les données des ressources de la BnF qui sont liées à ces autorités. https://data.bnf.fr/semanticweb https://data.bnf.fr/en/opendata","OAI-PMH, SPARQL, SRU, IIIF","JSON, JSON-LD, RDF","https://api.bnf.fr/fr/cgu","Licence ouverte de l’état - https://www.etalab.gouv.fr/licence-ouverte-open-licence/","CHO (Cultural Heritage Organizations)",,,"Bibliothèque nationale de France",,,"dcterms, rdf, rdfs, skos, foaf, rdaGr2","Metacatalog, Agregator",
" 06/03/2023 16:30:09","Open Data at the BnL","https://data.bnl.lu/apis/","In addition to raw datasets, the National Library of Luxembourg offers various web services, Application Programming Interfaces (APIs) and other interfaces to access and query data from its systems. The BnL is the coordinator of the bibnet.lu national network of Luxembourgish libraries. The InfoBib API offers information about each library in the network: address and opening times, contact information, access conditions etc. The BnL exposes structured metadata in the collective catalogue of the bibnet.lu library network using the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH). The OAI-PMH protocol is a means of exchanging metadata over the Internet between several institutions. The bibliographic records of the collective catalogue of the bibnet.lu library network are also searchable via the Z39.50 protocol. This protocol meets the ISO 23950 standard, “Information and documentation — Information retrieval (Z39.50) – Application service definition and protocol specification”. LiDa is BnL’s first open linked-data project. This project, in collaboration with the Centre national de littérature (National Literature Centre), linked authors and documents from the collective catalogue of the bibnet.lu network to those from the Dictionary of Luxembourgish Authors.","OAI-PMH","XML",,"https://data.bnl.lu/data/rights/","CHO (Cultural Heritage Organizations), mixed",,,"Bibliothèque nationale du Luxembourg","https://github.com/natliblux",,"dc, dcterms","Metacatalog",
" 06/03/2023 17:53:57","OAI interface","https://www.dnb.de/EN/oai","In order to use OAI to compare data between the German National Library and a service provider, the service provider must have implemented an OAI harvester. The OAI harvester calls itself repeatedly in a continuous loop. In doing so, it executes a ""ListRecords command"" limited to the dataset (catalogue) defined for the service provider. The time of the last retrieval is appended to the “ListRecords command“ using a time stamp. This guarantees that no change is missed. changes appear in the service provider’s database as soon as possible. no data irrelevant to the service provider is transported. Pentru accesul la SRU: https://www.dnb.de/EN/sru","OAI-PMH, SRU","XML, RDF","SRU: Standard (default): 10 data records per response Maximum: 100 data records per reply if ...&maximumRecords=100 specified (possible values 1 to 100) Call of further data records: ...&startRecord=101 (possible values 1 to 99,000)","All bibliographic data from the German National Library, the German Union Catalogue of Serials (ZDB), the metadata of the ISIL and library code list and the authority data from the Integrated Authority File (GND) is available free of charge for general re-use under Creative Commons Zero terms (CC0 1.0). Most of the holding data in the German Union Catalogue of Serials (ZDB) is also free for general re-use under CC0 1.0. A corresponding tag is incorporated into the record itself. The metadata and online interfaces are provided with no guarantee that they will be continuous, punctual, error-free or complete or that they do not violate third-party rights (e.g. personal rights and copyright).","CHO (Cultural Heritage Organizations)",,,"Deutsche Nationalbibliothek",,,"dcterms","Metacatalog",
" 06/03/2023 18:37:38","Data Catalog of the National Library of Finland","https://www.kiwi.fi/display/Datacatalog/APIs","Access open data sources maintained by the National Library of Finland http://data.nationallibrary.fi/","HTTP, OAI-PMH, SPARQL, Swagger, SRU","JSON, JSON-LD, RDF",,,"CHO (Cultural Heritage Organizations)",,,"National Library of Finland",,,,"Metacatalog",
" 08/03/2023 12:52:23","MUSEUMS VICTORIA COLLECTIONS","https://collections.museumsvictoria.com.au/developers","This site allows users to explore the natural sciences and humanities collections of Museums Victoria in Australia, featuring collections of zoology, geology, palaeontology, history, First Peoples and technology. Over 1.15 million records were presented at launch in 2015, accompanied by over 150,000 images. Our API is a set of methods based on restful ideas over HTTP. At this time it only supports the GET verb and responses are in JSON only. The root uri for the current version is located at https://collections.museumsvictoria.com.au/api ","HTTP","JSON",,"Open access to data, text content and images We wish to encourage reuse and sharing of content. The elements of each record that are defined as data carry a Creative Commons Zero (CC0) license. All the text content and information written by curators, collection managers, and others is released under a Creative Commons Attribution 4.0 International (CC- BY) licence. Images, sounds and audio-visual material are licensed separately to text. We apply open licenses wherever possible, including marking images as Public Domain as applicable. You will find over 100,000 openly licensed and public domain images in this site.",,,,,"https://github.com/museumsvictoria/collections-online",,,"Metacatalog",
" 09/03/2023 18:00:19","Library Database API","https://bibdb.libris.kb.se/api/lib","All data contained in the database is available through the API. However, contact information data requires sending with an API key as an extra header in the call. Contact Libris customer service if you need an API key. https://www.kb.se/samverkan-och-utveckling/libris/att-anvanda-librisdata/biblioteksdatabasens-api.html Swepub data is available to end users. The data can be retrieved freely as data dumps or via the OAI-PMH and SRU protocols. Open APIs provide access to data in the JSON format. There is also a lightweight API, Xsearch, that provides access to data via HTTP in a variety of formats. APIs also allow you to view full entries in Swepub MODS such as XML and Swepub BIBFRAME as JSON. https://www.kb.se/samverkan-och-utveckling/swepub/dataatkomst.html OAI PMH Original data such as Swepub retrieves it from the data providers without further data processing. About OAI PMH: Open Archives InitiativeLink to another site. Supported metadata formats: Dublin CoreLink to another website, opens in new window.( metadataPrefix = oai_dc ) MARCLink to another site.( metadataPrefix = marc ) MODSLink to another site.( metadataPrefix = mods ) Swepub MODS format specification v. 3.0Link to another website, opens in new window.( metadataPrefix = sweepub_mods ) Bas url: http://api.libris.kb.se/swepub/oaipmh/SWEPUB Update Frequency: Every night. Some data providers support selective autumning of new and updated data, while others perform a total reload periodically. This varies the date stamp for the latest data update. SRU SRU is an XML-based protocol for search. About SRU: Library of congress standardsLink to another site. Bas url: http://api.libris.kb.se/sru/swepub Xsearch lightweight API Xsearch is an HTTP-based lightweight API for accessing Swepub data in a variety of XML and text formats. Currently there is support for MARC-XML, Dublin Core, JSON, RIS, MODS and RDF. About Xsearch: Libris help pagesLink to another site. Bas url: http://libris.kb.se/xsearch?database=swepub APIs for the bibliometry, data processing, data status and subject classification services APIs used to retrieve data from Swepub's data warehouse for the bibliometry, data processing, data status and subject classification services are documented and freely available to users. An information API is also documented to retrieve organizational codes, output types and research topics used in Swepub. Note that the APIs do not have separate versioning but have the same version as the entire Swepub system. This means that with each new version of Swepub, the individual APIs may also have been changed. The APIs are backwards compatible as long as the version number of the Swepub system has the same full number ( major ). This means that any changes in minor versions are additions. Swepub's open APIs: https://bibliometri.swepub.kb.se/api/v1/apidocs/ APIs for full Swepub entries Duplicate entries according to Swepub MODS in the XML format contain the original record from the organization as it has been delivered to Swepub and converted to Swepub MODS. Full entries according to Swepub BIBFRAME in the JSON fomato are available both duplicated and deduplicated. Deducted mail contains the item that forms the basis of the merged item, enriched with information from the other duplicate items. Both contain Swepub's enrichment and normalization. MODS entry in Swepub: https://bibliometri.swepub.kb.se/api/v1/process/publications/[recordID]/original Duplicate BIBFRAME mail in Swepub: https://bibliometri.swepub.kb.se/api/v1/process/publications/[recordID] Deducted BIBFRAME mail in Swepub: https://bibliometri.swepub.kb.se/api/v1/bibliometrics/publications/[recordID] Replace [ recordID ] with the entry identifier from OAI-PMH. Tip: The Firefox browser displays JSON records in a clear way.","HTTP, OAI-PMH, Swagger, SRU","JSON, RDF, RIS",,"Swedish national bibliography and Swedish authority posts have been freely available since the summer of 2011 without restrictions. The license form is Creative Commons level 0. Swedish national bibliography and Swedish authority posts are a subset of Libris. The goal is to make the entire Libris database available under an open license. The data contains links to Wikipedia, DBPedia, LC Authorities ( name and subject word ) as well VIAF. VIAF was also used to find some of the links. There are two ways to access this data, either in Atom format or through the OAI-PMH protocol.",,,,"Kungliga Biblioteket",,"LIBRIS","dc","Metacatalog",
" 14/03/2023 18:37:20","The OpenAIRE APIs","https://graph.openaire.eu/develop/","Open Science is gradually becoming the modus operandi in research practices, affecting the way researchers collaborate and publish, discover, and access scientific knowledge. Scientists are increasingly publishing research results beyond the article, to share all scientific products (metadata and files) generated during an experiment, such as datasets, software, experiments. They publish in scholarly communication data sources (e.g. institutional repositories, data archives, software repositories), rely where possible on persistent identifiers (e.g. DOI, ORCID, Grid.ac, PDBs), specify semantic links to other research products (e.g. supplementedBy, citedBy, versionOf), and possibly to projects and/or relative funders. By following such practices, scientists are implicitly constructing the Global Open Science Graph, where by ""graph"" we mean a collection of objects interlinked by semantic relationships. The OpenAIRE Graph includes metadata and links between scientific products (e.g. literature, datasets, software, and ""other research products""), organizations, funders, funding streams, projects, communities, and (provenance) data sources - the details of the graph data model can be found in Zenodo.org. The Graph is available and obtained as an aggregation of the metadata and links collected from ~70.000 trusted sources, further enriched with metadata and links provided by: OpenAIRE end-users, e.g. researchers, project administrators, data curators providing links from scientific products to projects, funders, communities, or other products; OpenAIRE Full-text mining algorithms over around ~10Mi Open Access article full-texts; Research infrastructure scholarly services, bridged to the graph via OpenAIRE, exposing metadata of products such as research workflows, experiments, research objects, software, etc.. The Broker Service is available to use via the OpenAIRE Content Provider Dashboard. Thanks to the Broker, repositories, publishers or aggregators can exchange metadata and enrich their local metadata collection by subscribing to notifications of different types. The Broker is able to notify providers when the OpenAIRE Graph contains information that is not available in the original collection of the data source. https://api.openaire.eu/broker/swagger-ui/index.html The default format of delivered records is oaf (OpenAire Format - current version 1.0) https://graph.openaire.eu/develop/response-metadata-format.html","HTTP, Swagger","XML","FALSE","OpenAIRE Graph license is CC-BY: the records returned by the service can be freely re-used by commercial and non-commercial partners under CC-BY license, hence as long as OpenAIRE is acknowledged as a content provider.","Natural sciences (chemistry, earth sciences, physics, cosmology, biology, paleontology), Social sciences (economy, sociology, psychology, political science, law), Humanities (linguistics, philology, art, literature, history), Applied science (medical science, informatics, space)",,,"OpenAIRE",,,"oaf","Agregator",
" 01/04/2023 19:41:25","Semantic Scholar Academic Graph API","https://www.semanticscholar.org/product/api#Documentation","We provide the RESTful Semantic Scholar Academic Graph (S2AG) API as a service to the global research community. The API is a reliable on-demand source of data about authors, papers, citations, venues, and more that allows you to link directly to the corresponding page on semanticscholar.org for more information. Currently the S2AG API supports Paper and Author Lookup, Conflict of Interest detection, Conference Reviewer Match, SPECTER embeddings, and SUPP.AI annotations. We are actively developing new features based on user demand. https://api.semanticscholar.org/api-docs/graph https://api.semanticscholar.org/api-docs/peer-review This service of the Semantic Scholar API provides utilites to help conference organizers with the problem of assigning reviewers to conference submissions. It provides the following capabilities: Detection of conflict of interest, based on co-author relationships Computation of a matching score between a reviewer and a submission's topic, based on the reviewer's publication history For authorization to use this service, visit https://www.semanticscholar.org/product/api#Partner-Form. In the Project Description, note that you want to use the Peer Review API.","HTTP, Swagger","JSON",,"https://api.semanticscholar.org/license/","mixed",,,"Allen Institute for AI",,"S2AG",,"Agregator",
" 02/04/2023 10:37:14","{ NASA APIs }","https://api.nasa.gov/","The objective of this site is to make NASA data, including imagery, eminently accessible to application developers. This catalog focuses on broadly useful and user friendly APIs and does not hold every NASA API. One of the most popular websites at NASA is the Astronomy Picture of the Day. In fact, this website is one of the most popular websites across all federal agencies. It has the popular appeal of a Justin Bieber video. This endpoint structures the APOD imagery and associated metadata so that it can be repurposed for other applications. In addition, if the concept_tags parameter is set to True, then keywords derived from the image explanation are returned. These keywords could be used as auto-generated hashtags for twitter or instagram feeds; but generally help with discoverability of relevant imagery. The full documentation for this API can be found in the APOD API Github repository. NeoWs (Near Earth Object Web Service) is a RESTful web service for near earth Asteroid information. With NeoWs a user can: search for Asteroids based on their closest approach date to Earth, lookup a specific Asteroid with its NASA JPL small body id, as well as browse the overall data-set.","HTTP","JSON","Limits are placed on the number of API requests you may make using your API key. Rate limits may vary by service, but the defaults are: Hourly Limit: 1,000 requests per hour For each API key, these limits are applied across all api.nasa.gov API requests. Exceeding these limits will lead to your API key being temporarily blocked from making further requests. The block will automatically be lifted by waiting an hour. If you need higher rate limits, contact us.",,"Natural sciences (chemistry, earth sciences, physics, cosmology, biology, paleontology), Applied science (medical science, informatics, space)",,,"National Spatial Agency","https://github.com/nasa/api-docs",,,"Metacatalog",
" 02/04/2023 12:49:24","OpenCitations","https://opencitations.net/querying","OpenCitations is currently managed by the Research Centre for Open Scholarly Metadata, an independent research centre within the University of Bologna. The Research Centre has an International Board drawn from leaders within the main bibliographic stakeholder communities of relevance (librarians, bibliometricians, academics, data service providers, etc.) who have shown past solid commitment to open scholarship. The statutes of the Research Centre will ensure that OpenCitations' original aim of free provision of open bibliographic and citation data, services and software is maintained, and that OpenCitations as an organization cannot in future be taken over or controlled by commercial interests, nor become involved in political, regulatory, legislative or financial lobbying of any kind. OpenCitations SPARQL endpoints. OpenCitations made available a SPARQL endpoint for all the datasets released. When such a SPARQL endpoint is accessed with a browser, it shows an editor GUI generated with YASGUI. Of course, any SPARQL endpoint can additionally be queried using the SPARQL Protocol, e.g. via curl. The SPARQL endpoints available are: OpenCitations SPARQL endpoint of COCI; OpenCitations SPARQL endpoint of POCI, DOCI and CROCI; OpenCitations Meta SPARQL endpoint. OpenCitations REST APIs. All the data in any of the OpenCitations datasets can be retrieved by using an HTTP REST API. The rationale of making REST APIs available in addition to the SPARQL endpoints was to provide convenient access to the data included in the OpenCitations datasets for Web developers and users who are not necessarily experts in Semantic Web technologies. All the REST APIs made available by OpenCitations, has been implemented by means of RAMOSE, the Restful API Manager Over SPARQL Endpoints, which is a Python application that allows one to simply create a REST API over any SPARQL endpoint by means of a simple configuration file that execute a SPARQL query dependently of the particular API call specified. The REST APIs available are: OpenCitations Meta REST API; OpenCitations Indexes unifying REST API; COCI REST API; DOCI REST API; POCI REST API; CROCI REST API. If you are going to use the REST APIs within an application/code, we encourage you to first get the OpenCitations Access Token and specify it in the ""authorization"" header of your REST API call. Obtaining the token takes only a few seconds and needs to happen only once.","HTTP, SPARQL","JSON",,"The data held in any of the OpenCitations datasets are made freely available under a Creative Commons public domain dedication (CC0). The text of the web pages that comprise the OpenCitations web site is made freely available under a Creative Commons Attribution 4.0 International Public License. The software developed by OpenCitations for implementing all the services is made freely available on GitHub under the ISC License.",,,,"Research Centre for Open Scholarly Metadata","https://github.com/opencitations/metadata/",,"dcterms, rdfs, owl, foaf","Metacatalog, Agregator",
" 29/04/2023 11:40:32","re3data.org","https://www.re3data.org/api/doc","re3data.org supports the retrievial of its content via API. Currently the platform offers a simple open search implementation as well as a first version of a RESTful interface. Despite having HATEOAS in focus for the RESTful interface, the different APIs are versioned via URL and not via Accept-Header. This should make it easier to consume the data, since the client can be less smart. The interfaces can be accessed via https://www.re3data.org/api/<api identifier>","HTTP","XML",,"Except where otherwise noted, content on this site is licensed under a Creative Commons Attribution 4.0 International License","mixed",,,,,,,"Metacatalog",
" 29/04/2023 13:20:50","Digital Bodleian Search and Data","https://digital.bodleian.ox.ac.uk/developer/","Digital Bodleian first launched in 2015 with the aim of bringing together digitized content from the Bodleian Libraries’ extraordinary and rich collections into a single portal. The Bodleian Libraries have been digitizing content since the early 1990s and Digital Bodleian was and is designed to enable access to that content for the widest possible audience. Digital Bodleian quickly established itself as a key resource for research and teaching based on Bodleian Libraries’ collections, and usage has steadily increased over the years since launch. The site has relaunched, in late 2020, with a new and improved search and discovery interface, which aims to Facilitate improved searching, filtering and browsing of collection items Improve display of item-level metadata Enable viewing and download of highest-quality resolution images for the purposes of non-commercial research and teaching Improve curation and delivery of Bodleian Libraries content, as well as the delivery of content on behalf of partners such as Oxford’s colleges and Digital Humanities projects The relaunched Digital Bodleian benefited from funding from the Oxford University GLAM Digital Strategy Implementation Programme, and the technical redevelopment was completed in partnership with text & bytes LLC. Digital Bodleian Data API Specification (1.0.0) https://digital.bodleian.ox.ac.uk/developer/data/ Digital Bodleian IIIF API Specification (2.0.0) https://digital.bodleian.ox.ac.uk/developer/iiif/","HTTP, Swagger","JSON-LD",,"https://digital.bodleian.ox.ac.uk/terms/","CHO (Cultural Heritage Organizations)",,,"The Bodleian Libraries","https://github.com/bodleian",,,"Metacatalog",
" 20/05/2023 13:13:08","OpenAlex API","https://api.openalex.org/","OpenAlex is a fully open catalog of the global research system. It's named after the ancient Library of Alexandria and made by the nonprofit OurResearch. The OpenAlex dataset describes scholarly entities and how those entities are connected to each other. Types of entities include works, authors, sources, institutions, concepts, publishers, and funders. Together, these make a huge web (or more technically, heterogeneous directed graph) of hundreds of millions of entities and billions of connections between them all. The API is the primary way to get OpenAlex data. It's free and requires no authentication. The daily limit for API calls is 100,000 requests per user per day. For best performance, add your email to all API requests, like mailto=example@domain.com. OpenAlex offers an open replacement for industry-standard scientific knowledge bases like Elsevier's Scopus and Clarivate's Web of Science. Compared to these paywalled services, OpenAlex offers significant advantages in terms of inclusivity, affordability, and avaliability. https://docs.openalex.org/","HTTP","JSON","The API is limited to 100,000 calls per day. If you need more, simply drop us a line at support@openalex.org. There is a burst rate limit of 10 requests per second. So calling multiple requests at the same time could lead to errors with code 429. If you're calling the API with a list of IDs, using the OR syntax will save a lot of time and likely reduce any 429 errors. Check out our tutorial on how to do that with DOIs.","The website, API, and data snapshot are all available at no charge. The data is licensed as CC0 so it is free to use and distribute. As a nonprofit, making this data free and open is part of our mission.","mixed",,,"https://ourresearch.org/","https://github.com/ourresearch",,,"Agregator",
" 20/05/2023 13:41:32","Europe PMC","https://europepmc.org/developers","Europe PMC is hosted by EMBL’s European Bioinformatics Institute (EMBL-EBI), an international, innovative and interdisciplinary research organisation which aims to make the world’s public biological data freely available to the scientific community. Europe PMC is partnered with PubMed Central (PMC), and endorsed and supported by a group of international science funders as their repository of choice. Europe PMC mission and objectives ""To build open, full-text scientific literature resources and support innovation and discovery by engaging users, enabling contributors, and integrating related research outputs."" https://europepmc.org/About The Europe PMC RESTful Web Service gives you access to over 33 million publications from various sources, including PubMed, Agricola, the European Patents Office (EPO) and the National Institute for Clinical Excellence (NICE). Use the Web Service to access 4.5 million full text articles and 1.8 million open access articles Database cross-references to a number of databases, including UniProt, the European Nucleotide Archive (ENA) and more. Reference lists for more than 12.5 million publications. Citation counts and a citation network. Text-mined terms from full text articles, including accession numbers, chemicals, diseases, genes and proteins, Gene Ontology terms, and organisms. Use the Annotations API to access annotations in abstracts and full text articles, with links to related database records. For background information on Europe PMC content refer to the SOAP Web Service Reference Guide (Sections 2 and 3) and What am I searching on Europe PMC? The search module can be used to query search fields. Refer to the fields module, the search syntax reference or the SOAP Web Service Reference Guide for a list of available fields. Output response formats can be XML or JSON. In the case of the 'search' module a Dublin Core format is also available. Two versions of the RESTful web service are simultaneously available. This approach to release management allows users to prepare for a new version, rather than having to immediately respond to a version change. Join the Europe PMC web service users' Google group to receive notifications about web service releases. Construct your URL for a fields request as follows for the production version: GET https://www.ebi.ac.uk/europepmc/webservices/rest/fields Construct your URL for a fields request as follows for the test version of the web service: GET https://www.ebi.ac.uk/europepmc/webservices/test/rest/fields Grants RESTful (Grist) API The Grist database contains details of grants awarded by the Europe PMC Funders. SOAP web service A Simple Object Access Protocol (SOAP) based service to retrieve data from our publication database. You can search for metadata and full text articles in Europe PMC. The service has been modularised, and allows users to pick and choose the components required, making information retrieval more efficient. The following points describe how to use the Web Service, and provide example clients. OAI service The Europe PMC OAI service, (Europe PMC-OAI) provides access to metadata of all items in the Europe PMC archive, as well as to the full text of a large subset of these items. Europe PMC-OAI is an implementation of the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH), a standard for retrieving metadata from digital document repositories. Visit the Open Archives Initiative site for more information about the protocol and other activities of the OAI group. PMC-OAI supports OAI-PMH version 2.0. It does not support earlier versions of the protocol. Most of the items in this archive are copyright protected, with copyright held by the author(s) or the depositing journal. In general, the OAI service cannot be used to retrieve the full text of articles in PMC. The only exceptions to this policy are for articles that are in the public domain and those that are made available under an Open Access provision (see the PMC Open Access subset for specifics). Europe PMC-OAI details Base URL: http://europepmc.org/oai.cgi? Europe PMC OAI verbs Europe PMC metadata formats Europe PMC sets Access to full text Use the metadataPrefix =pmc to get the full text. e.g. http://europepmc.org/oai.cgi?verb=ListRecords&from;=2007-10-01&metadataPrefix;=pmc The parameter set=pmc-open identifies the complete collection of items in PMC for which the full text may be harvested. e.g. http://europepmc.org/oai.cgi?verb=ListRecords&metadataPrefix;=pmc&set;=pmc-open Retrieve a record by identifier. e.g. http://europepmc.org/oai.cgi?verb=GetRecord&metadataPrefix;=pmc&identifier;=oai:europepmc.org:2654146 Supported data formats The U.S. National Library of Medicine (NLM) Journal Archiving and Interchange XML format, for metadata or full-text article records. Complete documentation is available. Use metadataPrefix=pmc_fm for metadata, and metadataPrefix=pmc to get the full text of an item. Dublin Core format, for metadata only. Use metadataPrefix=oai_dc.  Automatic Segmentation of Large Result Sets If a ListIdentifiers request results in more than 1,000 hits, PMC-OAI will return the first 1,000 with a <resumptionToken> that can be used to get the remaining items. If a ListRecords request results in more than 25 hits for PMC full text, 50 hits for PMC metadata or 250 hits for Dublin Core format metadata, PMC-OAI will return the first 25, 50 or 250 records, respectively, with a <resumptionToken>. ","HTTP, OAI-PMH, SOAP","XML, JSON","These protocols provide access to Open Access content and metadata. It is not permissible to use any kind of automated process to bulk download other content from Europe PMC.",,"Natural sciences (chemistry, earth sciences, physics, cosmology, biology, paleontology), Applied science (medical science, informatics, space)",,,"Europe PMC is a service of the Europe PMC Funders' Group, in partnership with the European Bioinformatics Institute; and in cooperation with the National Center for Biotechnology Information at the U.S. National Library of Medicine (NCBI/NLM) . It includes content provided to the PMC International archive by participating publishers.","https://github.com/EuropePMC",,"dc, dcterms, dcmitype","Agregator",
" 20/05/2023 19:40:59","OpenCitations",,"OpenCitations is an independent not-for-profit infrastructure organization for open scholarship dedicated to the publication of open bibliographic and citation data by the use of Semantic Web (Linked Data) technologies. It is also engaged in advocacy for open citations, particularly in its role as a key founding member of the Initiative for Open Citations (I4OC). For administrative convenience, OpenCitations is managed by the Research Centre for Open Scholarly Metadata at the University of Bologna. OpenCitations espouses fully the founding principles of Open Science. It complies with the FAIR data principles by Force11 that data should be findable, accessible, interoperable and re-usable, and it complies with the recommendations of I4OC that citation data in particular should be structured, separable, and open. On the latter topic, OpenCitations has recently published a formal definition of an Open Citation, and has launched a system for globally unique and persistent identifiers (PIDs) for bibliographic citations – Open Citation Identifiers (OCIs). OpenCitations SPARQL endpoints. OpenCitations made available a SPARQL endpoint for all the datasets released. When such a SPARQL endpoint is accessed with a browser, it shows an editor GUI generated with YASGUI. Of course, any SPARQL endpoint can additionally be queried using the SPARQL Protocol, e.g. via curl. OpenCitations REST APIs. All the data in any of the OpenCitations datasets can be retrieved by using an HTTP REST API. The rationale of making REST APIs available in addition to the SPARQL endpoints was to provide convenient access to the data included in the OpenCitations datasets for Web developers and users who are not necessarily experts in Semantic Web technologies. All the REST APIs made available by OpenCitations, has been implemented by means of RAMOSE, the Restful API Manager Over SPARQL Endpoints, which is a Python application that allows one to simply create a REST API over any SPARQL endpoint by means of a simple configuration file that execute a SPARQL query dependently of the particular API call specified. The REST APIs available are: OpenCitations Meta REST API; OpenCitations Indexes unifying REST API; COCI REST API; DOCI REST API; POCI REST API; CROCI REST API. REST API itself has been created using RAMOSE, the Restful API Manager Over SPARQL Endpoints created by Silvio Peroni, which is licensed with an ISC license.","HTTP, SPARQL","JSON, CSV",,,"mixed",,,,,,,"Agregator",
" 05/06/2023 15:06:51","DOAJ API","https://doaj.org/api/","DOAJ is a unique and extensive index of diverse open access journals from around the world, driven by a growing community, committed to ensuring quality content is freely available online for everyone. DOAJ's mission is to increase the visibility, accessibility, reputation, usage and impact of quality, peer-reviewed, open access scholarly research journals globally, regardless of discipline, geography or language. https://doaj.org/api/v3/docs https://groups.google.com/g/doaj-public-api","HTTP, OAI-PMH, Swagger","XML, JSON","Is there an upload limit for uploading articles, or a rate limit? No, there is no limit set on how many articles you can upload, but we do have a rate limit. See below. There are two ways to upload articles to DOAJ: One by one via the Article CRUD API. This allows one article at a time but it should be possible to upload 1-2 per second, or more if you have multiple IP addresses sending them at once. In batches using the Article Bulk API (only for authenticated users). There are no limits to how many articles are uploaded in a batch. However, processing happens synchronously so you may encounter a timeout based on how long the articles take to process in our system. The timeout is set very high: our server has 10 minutes to respond before the web server closes the connection. Your client may drop the connection sooner, however. Keep the batch sizes small to help mitigate this. We recommend around 600 kilobytes. There is a rate limit of two requests per second on all API routes. ""Bursts"" are permitted, which means up to five requests per user are queued by the system and are fulfilled in turn so long as they average out to two requests per second overall.","DOAJ recommends the use of Creative Commons licenses to inform readers how published content can be used. We do not specify which license should be used. You may select the Creative Commons license that best meets the needs of the journal, or give authors the option to choose a license for their paper. You may not add to or adapt the terms of a Creative Commons license. This means you cannot impose restrictions on authors or users that conflict with the provisions of the license. Creative Commons licenses state clearly that ""For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply"". In some cases, we will accept a publisher's own license if it is broadly equivalent to one of the Creative Commons licenses. https://doaj.org/apply/copyright-and-licensing/","mixed",,,"DOAJ","https://github.com/DOAJ",,,"Metacatalog, Agregator",
" 11/06/2023 13:55:39","Open Library","https://openlibrary.org/dev/docs/restful_api","Open Library is an initiative of the Internet Archive, a 501(c)(3) non-profit, building a digital library of Internet sites and other cultural artifacts in digital form. Other projects include the Wayback Machine, archive.org and archive-it.org. Open Library is an open project: the software is open, the data are open, the documentation is open, and we welcome your contribution. Open Library offers a suite of APIs to help developers get up and running with our data. This includes RESTful APIs, which make Open Library data availabile in JSON, YAML and RDF/XML formats. There's also an earlier, now deprecated JSON API which is preserved for backward compatibility. Books API - Retrieve a specific work or edition by identifier Authors API - Retrieve an author and their works by author identifier Subjects API - Fetch books by subject name Search API - Search results for books, authors, and more Search inside API - Search for matching text within millions of books Partner API - Formerly the ""Read"" API, fetch one or more books by library identifiers (ISBNs, OCLC, LCCNs) Covers API - Fetch book covers by ISBN or Open Library identifier Recent Changes API - Programatic access to changes across Open Library Lists API - Reading, modifying, or creating user lists Bulk Access Please do not use our APIs for bulk download of Open Library data because this affects our ability to serve patrons. We make our data publicly available each month for partners. If you want a dump of complete data, please read about your Bulk Download options, or email us at openlibrary@archive.org. More APIs Did you know, nearly every page on Open Library is or has an API. You can return structured bibliographic data for any page by adding a .rdf/.json/.yml extension to the end of any Open Library identifier. For instance: https://openlibrary.org/works/OL15626917W.json or https://openlibrary.org/authors/OL33421A.json. Many pages, such as the Books, Authors, and Lists, will include links to their RDF and JSON formats. https://openlibrary.org/developers/api https://documenter.getpostman.com/view/10371902/SzS1U8oz","HTTP","XML, JSON",,,"mixed",,,"Internet Archive","https://github.com/internetarchive/openlibrary",,"dc","Metacatalog",
" 11/06/2023 14:20:01","WorldCat Search API","https://www.oclc.org/developer/api/oclc-apis/worldcat-search-api.en.html","The WorldCat Search API 2.0 supports much of the core functionality of the original WorldCat Search API in a streamlined JSON format. The API also allows enhanced access to local holdings record data across the cooperative, including shared print retention information. What you get Search WorldCat and retrieve bibliographic records for cataloged items such as books, videos, music and more in WorldCat. Retrieve single bibliographic records based on OCLC number, ISBN, ISSN, and other identifiers. Find out about libraries that hold an item based on OCLC number, ISBN, ISSN, and other identifiers. Find out about libraries that have committed to retain an item based on OCLC number, ISBN, ISSN, and other identifiers. developer.api.oclc.org is the new home for OCLC API documentation. So, if you’re ready to get to the technical details of OCLC APIs, you’re in the right place. Find documentation for API URLs, content negotiation options, response formats, sample requests, and more by selecting an API from the left menu. We have also added code examples to make getting started easier than ever. Ex: https://developer.api.oclc.org/citations-api https://www.oclc.org/developer/home.en.html https://www.oclc.org/developer/support/faq.en.html","HTTP, SRU","XML, JSON","We set limits on API usage that meet our typical member's needs while insuring sufficient network and server resources are available for everyone. For example, the WorldCat Search API v1 has a rolling 24 hour limit of 50,000 requests.","https://policies.oclc.org/en/copyright.html","mixed",,,"OCLC","https://github.com/OCLC-Developer-Network",,,"Metacatalog",
" 11/06/2023 15:22:51","Alma REST APIs","https://developers.exlibrisgroup.com/alma/apis/","https://developers.exlibrisgroup.com/alma/integrations/","HTTP, SWORD, Swagger, SRU","JSON","The APIs and API Materials are made available for use and access by Members that are customers of the relevant Ex Libris programs and services for their internal purposes and to other developers that can certify that they will not, on their own or on another party’s behalf, use or access such APIs or API Materials for any commercial purpose, including, without limitation, in connection with the marketing, sale, distribution, development or availability of any commercial third party products or services. Any use or access of the APIs or API Materials for commercial purposes or by or on behalf of third parties that are marketing, developing or distributing commercially available products or services is strictly prohibited without a separate license agreement signed by Ex Libris. https://developers.exlibrisgroup.com/about/terms/",,"mixed",,,"Clarivate (ExLibris)",,,,"Agregator",
" 11/06/2023 18:35:08","Research Organization Registry","https://ror.readme.io/docs/rest-api","The ROR REST API allows users to retrieve, search, and filter the organizations indexed in ROR. The API is built with Django, indexing and search is enabled by Elasticsearch, and results are returned as JSON. ROR's current data structure (aka its ""metadata schema"" or ""JSON schema"") is based on Digital Science's GRID, which provided the original seed data for the registry. GRID retired its public releases as of 16 Sep 2021, and ROR began managing its data independently from GRID in March 2022. The current ROR metadata schema inherited from GRID in 2019 is now unofficially known as version 1.0. After two rounds of community feedback in 2022/2023, we are planning metadata metadata schema version 2.0 for release in late 2023. Read more about ROR's plans for metadata schema versioning. To suggest a change to the ROR data structure, please file a Schema Change Request on the ROR Roadmap -- but before you do, please see whether your desired change is already planned in ROR metadata schema 2.0.","HTTP","JSON","No registration is required to use the ROR API, but note that the rate limit is a maximum of 2000 requests in a 5-minute period, and API traffic can be quite heavy at popular times like midnight UTC. If you need to make more requests or want to ensure faster response times, you can also run the entire ROR API locally in Docker. See the README on the ROR API GitHub repository for instructions on running the ROR API locally.","ROR data is freely and openly available without any restrictions under the Creative Commons CC0 1.0 Universal Public Domain dedication. ROR code is openly available on Github under a MIT License.","mixed",,,"California Digital Library, Crossref, and DataCite","https://github.com/ror-community/ror-api","ROR",,"Agregator",
" 11/06/2023 18:50:03","Sherpa APIs","https://v2.sherpa.ac.uk/api/","The Sherpa APIs (application programming interfaces) provide access to the functionality and datasets that Sherpa Services operate across. Swagger: https://app.swaggerhub.com/apis/gobfrey/v2.sherpa-api/2.2","HTTP, Swagger","JSON",,"We allow most of our website and its contents to be reused under the terms of a Creative Commons License (CC BY-NC-SA). https://www.jisc.ac.uk/website/copyright?loc=cc",,,,"JISC",,,,"Agregator",
" 21/06/2023 21:33:49","DataCite REST API","https://support.datacite.org/docs/api","The DataCite REST API allows any user to retrieve, query and browse DataCite DOI metadata records. In addition, DataCite Repositories can register DOIs and DataCite Members can manage Repositories and prefixes via the API. The API is generally RESTful and returns results in JSON, as the API follows the JSONAPI specification. The retrieve, query and browse functions do not require authentication, but the DataCite Member and Repository functions do require authentication with your DataCite Member or Repository ID. Other alternatives to retrieve, query and browse DataCite DOI metadata records include the DataCite OAI-PMH service and the DataCite Commons service. OAI-PMH is used primarily for bulk harvesting of metadata, and DataCite Search – which uses the DataCite REST API under the hood – provides a web interface to retrieve, query and browse DataCite metadata records. As of December 2019 the REST API is split into two versions: a Public API and a Member API. These two APIs use exactly the same URLs (starting with https://api.datacite.org), run exactly the same code, and provide exactly the same public data, the only difference being that traffic is directed to a different set of servers if users authenticate as a member. The DataCite status page page reflects this change, you can now see separate metrics (both response time and request count) for the Public API and Member API. https://blog.datacite.org/announcing-member-api/","HTTP, OAI-PMH","JSON","https://support.datacite.org/docs/apis","https://creativecommons.org/licenses/by/4.0/","mixed",,,"DataCite",,,,"Metacatalog",
" 21/06/2023 21:41:46","The OpenAIRE APIs","https://graph.openaire.eu/develop/overview.html","The OpenAIRE APIs can be accessed over HTTPS both by authenticated and non authenticated requests. Currently, there is an adjustment period until October 2022, when the rate limit for both authenticated and non authenticated requests is up to 7200 requests per hour. After this period we plan to significantly lower the rate limit of non authenticated requests up to 60 requests per hour. https://api.openaire.eu/broker/swagger-ui/index.html The OpenAIRE HTTP API allows developers to access metadata records of the OpenAIRE Graph by performing queries over publications, research data, and projects. The API is intended for metadata discovery and exploration only, that is it does not give direct access to publication files and it does not provide access to the whole information space: the number of total results returned by one query is limited to 10,000. For accessing the whole graph, developers are encouraged to use the OpenAIRE Graph dumps.","HTTP, Swagger","XML"," OPENAIRE APIS RATE LIMITS Not authenticated requests	up to 7200 requests per hour soon to decrease - please see above Authenticated requests	up to 7200 request per hour https://graph.openaire.eu/develop/overview.html#terms","OpenAIRE Graph license is CC-BY: the records returned by the service can be freely re-used by commercial and non-commercial partners under CC-BY license, hence as long as OpenAIRE is acknowledged as a data source.","mixed",,,"OpenAIRE",,,,"Agregator",
" 09/08/2023 10:48:19","bioRxiv API","https://api.biorxiv.org/","Preprint published article detail for specified server (bioRxiv or medRxiv) The format of the endpoint is https://api.biorxiv.org/pubs/[server]/[interval]/[cursor] or https://api.biorxiv.org/pubs/[server]/[DOI]/na/[format]. The help documentation is here: https://api.biorxiv.org/pubs/help. where 'server' can be either 'bioRxiv' or 'medRxiv' and 'interval' can be 1) two YYYY-MM-DD dates separted by '/' and 'cursor' is the start point which defaults to 0 if not supplied, or 2) a numeric value for the N most recent published articles, or 3) a numeric with the letter 'd' for the most recent N days of articles. Results are paginated with 100 articles served in a call. The 'cursor' value can be used to iterate through the result. For instance, https://api.biorxiv.org/pubs/medrxiv/2020-03-01/2020-03-30/5 will output metadata for the published version of the 100 medRxiv papers (if that many remain) published within the date range of 2020-03-01 to 2020-03-30 beginning from article 5. https://api.biorxiv.org/pubs/biorxiv/2020-03-01/2020-03-30/5 will do the same for bioRxiv papers for that period. https://api.biorxiv.org/pubs/[server]/[DOI]/na/[format] returns detail for a single manuscript. For instance, https://api.biorxiv.org/pubs/medrxiv/10.1101/2021.04.29.21256344 will output publication metadata for the biorxiv paper with DOI 10.1101/2021.04.29.21256344. Conversely, https://api.biorxiv.org/pubs/medrxiv/10.1371/journal.pone.0256482 will output publication metadata for the medRxiv paper with published DOI 10.1371/journal.pone.0256482. The 'messages' array in the output provides information about what is being displayed, including cursor value and count of items for the requested interval. The following metadata elements are returned: biorxiv_doi published_doi published_journal preprint_platform preprint_title preprint_authors preprint_category preprint_date published_date preprint_abstract preprint_author_corresponding preprint_author_corresponding_institution","HTTP, OAI-PMH","XML, JSON",,,"Natural sciences (chemistry, earth sciences, physics, cosmology, biology, paleontology)",,,"bioRxiv",,,,"Metacatalog",
" 09/08/2023 11:07:00","Altmetric","https://api.altmetric.com/","Altmetric does all the heavy lifting involved in extracting, disambiguating and collating mentions of scholarly content online, allowing you to focus on the bigger picture. The Details Page API provides you with programmatic access to the metrics data associated with articles, datasets, books and many other research outputs collected by Altmetric. With technical expertise, you can create your own ways of slicing and viewing our data, which gives you the freedom to complete more complex research, ask more granular questions of our data, and build custom visualizations. The major advantage is that API calls allow for programmatic, automated data flows, so that the data can be called into your own system, and remain current. The Details Page API focuses on a subset of the data we have about research outputs and mentions. It is a very powerful tool if you have the skills and resources needed to use it. The Details Page API allows you to find out everything you need to know about an individual research output (identified by DOI, PubMed ID or other scholarly identifier) or a wider range of results when querying using one or more journal ISSNs. This API works best when you have a list of output identifiers for which you want either the count of mentions Counts Only or the full text of mentions Full Access.","HTTP","JSON","Every day the Details Page API handles a large number of requests. To help manage the volume of these requests, limits are placed on the number of requests that can be made from a specific IP. These limits help us provide a reliable and dependable API service that serves the Altmetric community. If you are using the API without a key you can check the X-HourlyRateLimit-Limit and X-DailyRateLimit-Limit headers for the current limits. The X-HourlyRateLimit-Remaining and X-DailyRateLimit-Remaining headers will tell you how many calls you have remaining. When your rate limit has been exceeded, a 429 'Too many requests' error is returned by the API. When this occurs it is recommended that you examine HTTP headers above and pause requests until sufficient time has passed. If you find that you frequently hit the rate limit then you might want to consider throttling your requests or purchasing a commercial API key.","https://api.altmetric.com/licensing.html#licensing","mixed",,,"Altmetric LLC",,,,"Agregator",
" 09/08/2023 11:29:34","Unpaywall","https://unpaywall.org/products/api","An open database of 47,895,197 free scholarly articles. We harvest Open Access content from over 50,000 publishers and repositories, and make it easy to find, track, and use. The database snapshot, Simple Query Tool, REST API, and Data Feed products all return JSON-formatted data. For simplicity, that data is organized under the same schema in all cases; that schema is informally described on this page. Regardless of the source, each record returned consists of one DOI Object, containing resource metadata. Each DOI Object in turn contains a list of zero or more OA Location Objects. New fields may be added at any time. This won't be a problem for existing code in most cases since they will simply go unused, but you shouldn't rely on the number of fields being fixed.","HTTP","JSON","Please limit use to 100,000 calls per day. If you need faster access, you'll be better served by downloading the entire database snapshot for local access.","https://unpaywall.org/legal/terms-of-service",,,,,,,,"Agregator",
